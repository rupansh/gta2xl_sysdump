name: "SelfieNet v3.1"
input: "data"
input_dim: 1
input_dim: 3
input_dim: 256
input_dim: 256

layer {
    name: "ConvNdBackward1"
    type: "Convolution"
    bottom: "data"
    top: "ConvNdBackward1"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward2"
    type: "ReLU"
    bottom: "ConvNdBackward1"
    top: "ConvNdBackward1"
}
layer {
    name: "MaxPool2DBackward3"
    type: "Pooling"
    bottom: "ConvNdBackward1"
    top: "MaxPool2DBackward3"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConvNdBackward4"
    type: "Convolution"
    bottom: "MaxPool2DBackward3"
    top: "ConvNdBackward4"
    convolution_param {
        num_output: 16
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward5"
    type: "ReLU"
    bottom: "ConvNdBackward4"
    top: "ConvNdBackward4"
}
layer {
    name: "ConvNdBackward6"
    type: "Convolution"
    bottom: "ConvNdBackward4"
    top: "ConvNdBackward6"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward7"
    type: "ReLU"
    bottom: "ConvNdBackward6"
    top: "ConvNdBackward6"
}
layer {
    name: "ConvNdBackward9"
    type: "Convolution"
    bottom: "ConvNdBackward4"
    top: "ConvNdBackward9"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward10"
    type: "ReLU"
    bottom: "ConvNdBackward9"
    top: "ConvNdBackward9"
}
layer {
    name: "CatBackward11"
    type: "Concat"
    bottom: "ConvNdBackward6"
    bottom: "ConvNdBackward9"
    top: "CatBackward11"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward12"
    type: "Convolution"
    bottom: "CatBackward11"
    top: "ConvNdBackward12"
    convolution_param {
        num_output: 16
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward13"
    type: "ReLU"
    bottom: "ConvNdBackward12"
    top: "ConvNdBackward12"
}
layer {
    name: "ConvNdBackward14"
    type: "Convolution"
    bottom: "ConvNdBackward12"
    top: "ConvNdBackward14"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward15"
    type: "ReLU"
    bottom: "ConvNdBackward14"
    top: "ConvNdBackward14"
}
layer {
    name: "ConvNdBackward17"
    type: "Convolution"
    bottom: "ConvNdBackward12"
    top: "ConvNdBackward17"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward18"
    type: "ReLU"
    bottom: "ConvNdBackward17"
    top: "ConvNdBackward17"
}
layer {
    name: "CatBackward19"
    type: "Concat"
    bottom: "ConvNdBackward14"
    bottom: "ConvNdBackward17"
    top: "CatBackward19"
    concat_param {
        axis: 1
    }
}
layer {
    name: "MaxPool2DBackward20"
    type: "Pooling"
    bottom: "CatBackward19"
    top: "MaxPool2DBackward20"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConvNdBackward21"
    type: "Convolution"
    bottom: "MaxPool2DBackward20"
    top: "ConvNdBackward21"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward22"
    type: "ReLU"
    bottom: "ConvNdBackward21"
    top: "ConvNdBackward21"
}
layer {
    name: "ConvNdBackward23"
    type: "Convolution"
    bottom: "ConvNdBackward21"
    top: "ConvNdBackward23"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward24"
    type: "ReLU"
    bottom: "ConvNdBackward23"
    top: "ConvNdBackward23"
}
layer {
    name: "ConvNdBackward26"
    type: "Convolution"
    bottom: "ConvNdBackward21"
    top: "ConvNdBackward26"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward27"
    type: "ReLU"
    bottom: "ConvNdBackward26"
    top: "ConvNdBackward26"
}
layer {
    name: "CatBackward28"
    type: "Concat"
    bottom: "ConvNdBackward23"
    bottom: "ConvNdBackward26"
    top: "CatBackward28"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward29"
    type: "Convolution"
    bottom: "CatBackward28"
    top: "ConvNdBackward29"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward30"
    type: "ReLU"
    bottom: "ConvNdBackward29"
    top: "ConvNdBackward29"
}
layer {
    name: "ConvNdBackward31"
    type: "Convolution"
    bottom: "ConvNdBackward29"
    top: "ConvNdBackward31"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward32"
    type: "ReLU"
    bottom: "ConvNdBackward31"
    top: "ConvNdBackward31"
}
layer {
    name: "ConvNdBackward34"
    type: "Convolution"
    bottom: "ConvNdBackward29"
    top: "ConvNdBackward34"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward35"
    type: "ReLU"
    bottom: "ConvNdBackward34"
    top: "ConvNdBackward34"
}
layer {
    name: "CatBackward36"
    type: "Concat"
    bottom: "ConvNdBackward31"
    bottom: "ConvNdBackward34"
    top: "CatBackward36"
    concat_param {
        axis: 1
    }
}
layer {
    name: "MaxPool2DBackward37"
    type: "Pooling"
    bottom: "CatBackward36"
    top: "MaxPool2DBackward37"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConvNdBackward38"
    type: "Convolution"
    bottom: "MaxPool2DBackward37"
    top: "ConvNdBackward38"
    convolution_param {
        num_output: 48
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward39"
    type: "ReLU"
    bottom: "ConvNdBackward38"
    top: "ConvNdBackward38"
}
layer {
    name: "ConvNdBackward40"
    type: "Convolution"
    bottom: "ConvNdBackward38"
    top: "ConvNdBackward40"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward41"
    type: "ReLU"
    bottom: "ConvNdBackward40"
    top: "ConvNdBackward40"
}
layer {
    name: "ConvNdBackward43"
    type: "Convolution"
    bottom: "ConvNdBackward38"
    top: "ConvNdBackward43"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward44"
    type: "ReLU"
    bottom: "ConvNdBackward43"
    top: "ConvNdBackward43"
}
layer {
    name: "CatBackward45"
    type: "Concat"
    bottom: "ConvNdBackward40"
    bottom: "ConvNdBackward43"
    top: "CatBackward45"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward46"
    type: "Convolution"
    bottom: "CatBackward45"
    top: "ConvNdBackward46"
    convolution_param {
        num_output: 48
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward47"
    type: "ReLU"
    bottom: "ConvNdBackward46"
    top: "ConvNdBackward46"
}
layer {
    name: "ConvNdBackward48"
    type: "Convolution"
    bottom: "ConvNdBackward46"
    top: "ConvNdBackward48"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward49"
    type: "ReLU"
    bottom: "ConvNdBackward48"
    top: "ConvNdBackward48"
}
layer {
    name: "ConvNdBackward51"
    type: "Convolution"
    bottom: "ConvNdBackward46"
    top: "ConvNdBackward51"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward52"
    type: "ReLU"
    bottom: "ConvNdBackward51"
    top: "ConvNdBackward51"
}
layer {
    name: "CatBackward53"
    type: "Concat"
    bottom: "ConvNdBackward48"
    bottom: "ConvNdBackward51"
    top: "CatBackward53"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward54"
    type: "Convolution"
    bottom: "CatBackward53"
    top: "ConvNdBackward54"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward55"
    type: "ReLU"
    bottom: "ConvNdBackward54"
    top: "ConvNdBackward54"
}
layer {
    name: "ConvNdBackward56"
    type: "Convolution"
    bottom: "ConvNdBackward54"
    top: "ConvNdBackward56"
    convolution_param {
        num_output: 256
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward57"
    type: "ReLU"
    bottom: "ConvNdBackward56"
    top: "ConvNdBackward56"
}
layer {
    name: "ConvNdBackward59"
    type: "Convolution"
    bottom: "ConvNdBackward54"
    top: "ConvNdBackward59"
    convolution_param {
        num_output: 256
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward60"
    type: "ReLU"
    bottom: "ConvNdBackward59"
    top: "ConvNdBackward59"
}
layer {
    name: "CatBackward61"
    type: "Concat"
    bottom: "ConvNdBackward56"
    bottom: "ConvNdBackward59"
    top: "CatBackward61"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward62"
    type: "Convolution"
    bottom: "CatBackward61"
    top: "ConvNdBackward62"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward63"
    type: "ReLU"
    bottom: "ConvNdBackward62"
    top: "ConvNdBackward62"
}
layer {
    name: "ConvNdBackward64"
    type: "Convolution"
    bottom: "ConvNdBackward62"
    top: "ConvNdBackward64"
    convolution_param {
        num_output: 256
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward65"
    type: "ReLU"
    bottom: "ConvNdBackward64"
    top: "ConvNdBackward64"
}
layer {
    name: "ConvNdBackward67"
    type: "Convolution"
    bottom: "ConvNdBackward62"
    top: "ConvNdBackward67"
    convolution_param {
        num_output: 256
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward68"
    type: "ReLU"
    bottom: "ConvNdBackward67"
    top: "ConvNdBackward67"
}
layer {
    name: "CatBackward69"
    type: "Concat"
    bottom: "ConvNdBackward64"
    bottom: "ConvNdBackward67"
    top: "CatBackward69"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward70"
    type: "Convolution"
    bottom: "CatBackward69"
    top: "ConvNdBackward70"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward71"
    type: "Convolution"
    bottom: "ConvNdBackward70"
    top: "ConvNdBackward71"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward73"
    type: "Convolution"
    bottom: "CatBackward69"
    top: "ConvNdBackward73"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward74"
    type: "Convolution"
    bottom: "ConvNdBackward73"
    top: "ConvNdBackward74"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward175"
    type: "Eltwise"
    bottom: "ConvNdBackward71"
    bottom: "ConvNdBackward74"
    top: "AddBackward175"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward77"
    type: "Convolution"
    bottom: "AddBackward175"
    top: "ConvNdBackward77"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward78"
    type: "ReLU"
    bottom: "ConvNdBackward77"
    top: "ConvNdBackward77"
}
layer {
    name: "ConvNdBackward79"
    type: "Convolution"
    bottom: "ConvNdBackward77"
    top: "ConvNdBackward79"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward180"
    type: "Eltwise"
    bottom: "AddBackward175"
    bottom: "ConvNdBackward79"
    top: "AddBackward180"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward81"
    type: "Deconvolution"
    bottom: "AddBackward180"
    top: "ConvNdBackward81"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 4
        kernel_w: 4
        stride: 2
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "ThresholdBackward82"
    type: "ReLU"
    bottom: "ConvNdBackward81"
    top: "ConvNdBackward81"
}
layer {
    name: "ConvNdBackward84"
    type: "Convolution"
    bottom: "CatBackward36"
    top: "ConvNdBackward84"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward85"
    type: "Convolution"
    bottom: "ConvNdBackward84"
    top: "ConvNdBackward85"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward87"
    type: "Convolution"
    bottom: "CatBackward36"
    top: "ConvNdBackward87"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward88"
    type: "Convolution"
    bottom: "ConvNdBackward87"
    top: "ConvNdBackward88"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward189"
    type: "Eltwise"
    bottom: "ConvNdBackward85"
    bottom: "ConvNdBackward88"
    top: "AddBackward189"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward91"
    type: "Convolution"
    bottom: "AddBackward189"
    top: "ConvNdBackward91"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward92"
    type: "ReLU"
    bottom: "ConvNdBackward91"
    top: "ConvNdBackward91"
}
layer {
    name: "ConvNdBackward93"
    type: "Convolution"
    bottom: "ConvNdBackward91"
    top: "ConvNdBackward93"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward194"
    type: "Eltwise"
    bottom: "AddBackward189"
    bottom: "ConvNdBackward93"
    top: "AddBackward194"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward195"
    type: "Eltwise"
    bottom: "ConvNdBackward81"
    bottom: "AddBackward194"
    top: "AddBackward195"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward97"
    type: "Convolution"
    bottom: "AddBackward195"
    top: "ConvNdBackward97"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward98"
    type: "ReLU"
    bottom: "ConvNdBackward97"
    top: "ConvNdBackward97"
}
layer {
    name: "ConvNdBackward99"
    type: "Convolution"
    bottom: "ConvNdBackward97"
    top: "ConvNdBackward99"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1100"
    type: "Eltwise"
    bottom: "AddBackward195"
    bottom: "ConvNdBackward99"
    top: "AddBackward1100"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward101"
    type: "Deconvolution"
    bottom: "AddBackward1100"
    top: "ConvNdBackward101"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 4
        kernel_w: 4
        stride: 2
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "ThresholdBackward102"
    type: "ReLU"
    bottom: "ConvNdBackward101"
    top: "ConvNdBackward101"
}
layer {
    name: "ConvNdBackward104"
    type: "Convolution"
    bottom: "CatBackward19"
    top: "ConvNdBackward104"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward105"
    type: "Convolution"
    bottom: "ConvNdBackward104"
    top: "ConvNdBackward105"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward107"
    type: "Convolution"
    bottom: "CatBackward19"
    top: "ConvNdBackward107"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ConvNdBackward108"
    type: "Convolution"
    bottom: "ConvNdBackward107"
    top: "ConvNdBackward108"
    convolution_param {
        num_output: 32
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1109"
    type: "Eltwise"
    bottom: "ConvNdBackward105"
    bottom: "ConvNdBackward108"
    top: "AddBackward1109"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward111"
    type: "Convolution"
    bottom: "AddBackward1109"
    top: "ConvNdBackward111"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward112"
    type: "ReLU"
    bottom: "ConvNdBackward111"
    top: "ConvNdBackward111"
}
layer {
    name: "ConvNdBackward113"
    type: "Convolution"
    bottom: "ConvNdBackward111"
    top: "ConvNdBackward113"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1114"
    type: "Eltwise"
    bottom: "AddBackward1109"
    bottom: "ConvNdBackward113"
    top: "AddBackward1114"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1115"
    type: "Eltwise"
    bottom: "ConvNdBackward101"
    bottom: "AddBackward1114"
    top: "AddBackward1115"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward117"
    type: "Convolution"
    bottom: "AddBackward1115"
    top: "ConvNdBackward117"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward118"
    type: "ReLU"
    bottom: "ConvNdBackward117"
    top: "ConvNdBackward117"
}
layer {
    name: "ConvNdBackward119"
    type: "Convolution"
    bottom: "ConvNdBackward117"
    top: "ConvNdBackward119"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1120"
    type: "Eltwise"
    bottom: "AddBackward1115"
    bottom: "ConvNdBackward119"
    top: "AddBackward1120"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward121"
    type: "Deconvolution"
    bottom: "AddBackward1120"
    top: "ConvNdBackward121"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 4
        kernel_w: 4
        stride: 2
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "ThresholdBackward122"
    type: "ReLU"
    bottom: "ConvNdBackward121"
    top: "ConvNdBackward121"
}
layer {
    name: "ConvNdBackward124"
    type: "Convolution"
    bottom: "ConvNdBackward121"
    top: "ConvNdBackward124"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward125"
    type: "ReLU"
    bottom: "ConvNdBackward124"
    top: "ConvNdBackward124"
}
layer {
    name: "ConvNdBackward126"
    type: "Convolution"
    bottom: "ConvNdBackward124"
    top: "ConvNdBackward126"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1127"
    type: "Eltwise"
    bottom: "ConvNdBackward121"
    bottom: "ConvNdBackward126"
    top: "AddBackward1127"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "ConvNdBackward128"
    type: "Deconvolution"
    bottom: "AddBackward1127"
    top: "ConvNdBackward128"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 4
        kernel_w: 4
        stride: 2
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "ThresholdBackward129"
    type: "ReLU"
    bottom: "ConvNdBackward128"
    top: "ConvNdBackward128"
}
layer {
    name: "ConvNdBackward130"
    type: "Convolution"
    bottom: "ConvNdBackward128"
    top: "ConvNdBackward130"
    convolution_param {
        num_output: 2
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward131"
    type: "ReLU"
    bottom: "ConvNdBackward130"
    top: "ConvNdBackward130"
}
layer {
    name: "ConvNdBackward133"
    type: "Convolution"
    bottom: "ConvNdBackward130"
    top: "ConvNdBackward133"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "ThresholdBackward134"
    type: "ReLU"
    bottom: "ConvNdBackward133"
    top: "ConvNdBackward133"
}
layer {
    name: "ConvNdBackward135"
    type: "Convolution"
    bottom: "ConvNdBackward133"
    top: "ConvNdBackward135"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
    }
}
layer {
    name: "AddBackward1136"
    type: "Eltwise"
    bottom: "ConvNdBackward130"
    bottom: "ConvNdBackward135"
    top: "AddBackward1136"
    eltwise_param {
        operation: SUM
    }
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "AddBackward1136"
  top: "prob"
}
